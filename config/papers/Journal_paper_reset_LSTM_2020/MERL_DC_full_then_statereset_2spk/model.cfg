[hyper]
model_names = main_lstm reset_lstm outlayer

[main_lstm]
#type of architecture
architecture = dblstm
#number of neurons in the hidden layers
num_units = 600
#number of hidden layers
num_layers = 1
#input noise standart deviation
input_noise = 0.2
#dropout rate (keep probability)
dropout = 1.0
#the recurrent dropout rate (keep probability)
recurrent_dropout = 1.0
#wheter layer normalization should be applied
layer_norm = False

[reset_lstm]
#type of architecture
architecture = dbresetlstm
#number of neurons in the hidden layers
num_units = 600
#the reset cycle period
t_reset = 15
#wether the same amount of context should be used for input from both directions
symmetric_context = True
#number of hidden layers
num_layers = 1
#input noise standart deviation
input_noise = 0
#dropout rate (keep probability)
dropout = 1.0
#the recurrent dropout rate (keep probability)
recurrent_dropout = 1.0
#wheter layer normalization should be applied
layer_norm = False

[outlayer]
#type of architecture
architecture = linear
#the number of output dims (should be set automaticaly?)
output_dims = 2580
#input noise standart deviation
input_noise = 0
#dropout rate (keep probability)
dropout = 1.0
