[hyper]
model_names = main aux_lstm concat feedforward outlayer

[main]
#type of architecture
architecture = encoder_decoder_cnn
#wheter or not to use the bypass mechanism from encoder to decoder
bypass = unpool
# number of filters in the first layer. The next layer has twice the filters and so on
num_filters_1st_layer = 80
# number of filters in the output layer (should probably not need to be big, since only a couple of speakers can be active)
num_output_filters = 80
#the number of filters in layer l is num_filters_1st_layer*(fac_per_layer**l)
fac_per_layer = 1.0
#convolutional filter size in time dimension
filter_size_t = 12
#how much the filter size in time dimension should increase after each max pool operation. Typically chosen smaller than 1
filter_size_t_fac_after_pool = 0.8
#convolutional filter size in frequency dimension
filter_size_f = 15
#how much the filter size in frequency dimension should increase after each max pool operation. Typically chosen smaller than 1
filter_size_f_fac_after_pool = 0.8
#number of hidden layers in the encoder part
num_encoder_layers = 2
#number of hidden layers in the center part
num_centre_layers = 0
#do maxpooling over frequency dimension with shape 2 every 'f_pool_rate' encoder layers
f_pool_rate = 1
#do maxpooling over time dimension with shape 2 every 't_pool_rate' encoder layers
t_pool_rate = 3
#hidden unit activation
activation_fn = relu
#input noise standart deviation
input_noise = 0.2
#dropout rate (keep probability)
dropout = 1.0
#wheter layer normalization should be applied
layer_norm = False

[aux_lstm]
#type of architecture
architecture = dblstm
#number of neurons in the hidden layers
num_units = 344
#the number of neurons in layer l is num_units*(fac_per_layer**l)
fac_per_layer = 1.0
#number of hidden layers
num_layers = 4
#input noise standart deviation
input_noise = 0.2
#dropout rate (keep probability)
dropout = 1.0
#the recurrent dropout rate (keep probability)
recurrent_dropout = 1.0
#wheter layer normalization should be applied
layer_norm = False
#hidden unit activation
activation_fn = tanh

[concat]
#type of architecture
architecture = concat
# which inputs to concatenate
select_inputs = True True
# whether the 2 last dimensions should be flattened
flatten_last_2_dims = False False
# whether the a dimension should be expanded to match the shape of the first input
expand_dim_to_first_input = False True

[feedforward]
#type of architecture
architecture = feedforward
#the number of output dims
num_units = 400
#the number of neurons in layer l is num_units*(fac_per_layer**l)
fac_per_layer = 1.0
#number of hidden layers
num_layers = 3
#hidden unit activation
activation_func = relu
#input noise standart deviation
input_noise = 0
#dropout rate (keep probability)
dropout = 1.0

[outlayer]
#type of architecture
architecture = linear
#the number of output dims
output_dims = 20
#input noise standart deviation
input_noise = 0
#dropout rate (keep probability)
dropout = 1.0
